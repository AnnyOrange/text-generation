01/24/2024 09:37:22 - INFO - __main__ - ***** Running training *****
01/24/2024 09:37:22 - INFO - __main__ -   Num examples = 275
01/24/2024 09:37:22 - INFO - __main__ -   Num Epochs = 1
01/24/2024 09:37:22 - INFO - __main__ -   Instantaneous batch size per device = 2
01/24/2024 09:37:22 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 8
01/24/2024 09:37:22 - INFO - __main__ -   Gradient Accumulation steps = 4
01/24/2024 09:37:22 - INFO - __main__ -   Total optimization steps = 15
Steps:   0%|                                                                                                                                                                                        | 0/15 [00:00<?, ?it/s]C:\Users\30661\.conda\envs\torchgpu\lib\site-packages\diffusers\configuration_utils.py:139: FutureWarning: Accessing config attribute `num_train_timesteps` directly via 'DDIMScheduler' object attribute is deprecated. Please access 'num_train_timesteps' over 'DDIMScheduler's config object instead, e.g. 'scheduler.config.num_train_timesteps'.
  deprecate("direct config name access", "1.0.0", deprecation_message, standard_warn=False)
Steps:   0%|                                                                                                                                                              | 0/15 [00:03<?, ?it/s, lr=5e-6, step_loss=0.107]01/24/2024 09:37:26 - INFO - __main__ - Running validation...
 Generating 1 images with prompt: Transform the text in the image into Anny's style with a white background..
{'image_encoder'} was not found in config. Values will be initialized to default values.
                                                                                                                                                                                                                           Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of timbrooks/instruct-pix2pix.
Loading pipeline components...:   0%|                                                                                                                                                                | 0/7 [00:00<?, ?it/s]{'timestep_spacing', 'rescale_betas_zero_snr'} was not found in config. Values will be initialized to default values.
Loaded scheduler as EulerAncestralDiscreteScheduler from `scheduler` subfolder of timbrooks/instruct-pix2pix.                                                                                | 1/7 [00:00<00:02,  2.63it/s]
{'scaling_factor', 'force_upcast'} was not found in config. Values will be initialized to default values.
Loaded vae as AutoencoderKL from `vae` subfolder of timbrooks/instruct-pix2pix.
                                                                                                                                                                                                                           Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of timbrooks/instruct-pix2pix.
`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["id2label"]` will be overriden.                                                     | 4/7 [00:00<00:00,  7.67it/s]
`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["bos_token_id"]` will be overriden.
`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config["eos_token_id"]` will be overriden.
Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of timbrooks/instruct-pix2pix.
                                                                                                                                                                                                                           Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of timbrooks/instruct-pix2pix.
Loading pipeline components...: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:01<00:00,  5.03it/s]
Traceback (most recent call last):
  File "D:\AI\text-re\text-generation\finetune_img2img.py", line 1261, in <module>
    main()
  File "D:\AI\text-re\text-generation\finetune_img2img.py", line 1137, in main
    pipeline(
  File "C:\Users\30661\.conda\envs\torchgpu\lib\site-packages\torch\utils\_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "C:\Users\30661\.conda\envs\torchgpu\lib\site-packages\diffusers\pipelines\stable_diffusion\pipeline_stable_diffusion_img2img.py", line 1057, in __call__
    noise_pred = self.unet(
  File "C:\Users\30661\.conda\envs\torchgpu\lib\site-packages\torch\nn\modules\module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\30661\.conda\envs\torchgpu\lib\site-packages\torch\nn\modules\module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\30661\.conda\envs\torchgpu\lib\site-packages\accelerate\utils\operations.py", line 659, in forward
    return model_forward(*args, **kwargs)
  File "C:\Users\30661\.conda\envs\torchgpu\lib\site-packages\accelerate\utils\operations.py", line 647, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "C:\Users\30661\.conda\envs\torchgpu\lib\site-packages\torch\amp\autocast_mode.py", line 16, in decorate_autocast
    return func(*args, **kwargs)
  File "C:\Users\30661\.conda\envs\torchgpu\lib\site-packages\diffusers\models\unet_2d_condition.py", line 1081, in forward
    sample = self.conv_in(sample)
  File "C:\Users\30661\.conda\envs\torchgpu\lib\site-packages\torch\nn\modules\module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\30661\.conda\envs\torchgpu\lib\site-packages\torch\nn\modules\module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\30661\.conda\envs\torchgpu\lib\site-packages\torch\nn\modules\conv.py", line 460, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "C:\Users\30661\.conda\envs\torchgpu\lib\site-packages\torch\nn\modules\conv.py", line 456, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Given groups=1, weight of size [320, 8, 3, 3], expected input[2, 4, 9, 9] to have 8 channels, but got 4 channels instead
validate0 1