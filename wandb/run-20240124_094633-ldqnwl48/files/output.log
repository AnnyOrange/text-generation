01/24/2024 09:46:39 - INFO - __main__ - ***** Running training *****
01/24/2024 09:46:39 - INFO - __main__ -   Num examples = 275
01/24/2024 09:46:39 - INFO - __main__ -   Num Epochs = 1
01/24/2024 09:46:39 - INFO - __main__ -   Instantaneous batch size per device = 2
01/24/2024 09:46:39 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 8
01/24/2024 09:46:39 - INFO - __main__ -   Gradient Accumulation steps = 4
01/24/2024 09:46:39 - INFO - __main__ -   Total optimization steps = 15
Steps:   0%|                                                                                                                                                      | 0/15 [00:00<?, ?it/s]C:\Users\30661\.conda\envs\torchgpu\lib\site-packages\diffusers\configuration_utils.py:139: FutureWarning: Accessing config attribute `num_train_timesteps` directly via 'DDIMScheduler' object attribute is deprecated. Please access 'num_train_timesteps' over 'DDIMScheduler's config object instead, e.g. 'scheduler.config.num_train_timesteps'.
  deprecate("direct config name access", "1.0.0", deprecation_message, standard_warn=False)
Traceback (most recent call last):
  File "D:\AI\text-re\text-generation\finetune_img2img.py", line 1261, in <module>
    main()
  File "D:\AI\text-re\text-generation\finetune_img2img.py", line 1054, in main
    model_pred = unet(
  File "C:\Users\30661\.conda\envs\torchgpu\lib\site-packages\torch\nn\modules\module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\30661\.conda\envs\torchgpu\lib\site-packages\torch\nn\modules\module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\30661\.conda\envs\torchgpu\lib\site-packages\accelerate\utils\operations.py", line 659, in forward
    return model_forward(*args, **kwargs)
  File "C:\Users\30661\.conda\envs\torchgpu\lib\site-packages\accelerate\utils\operations.py", line 647, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "C:\Users\30661\.conda\envs\torchgpu\lib\site-packages\torch\amp\autocast_mode.py", line 16, in decorate_autocast
    return func(*args, **kwargs)
  File "C:\Users\30661\.conda\envs\torchgpu\lib\site-packages\diffusers\models\unet_2d_condition.py", line 1081, in forward
    sample = self.conv_in(sample)
  File "C:\Users\30661\.conda\envs\torchgpu\lib\site-packages\torch\nn\modules\module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\30661\.conda\envs\torchgpu\lib\site-packages\torch\nn\modules\module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Users\30661\.conda\envs\torchgpu\lib\site-packages\torch\nn\modules\conv.py", line 460, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "C:\Users\30661\.conda\envs\torchgpu\lib\site-packages\torch\nn\modules\conv.py", line 456, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Given groups=1, weight of size [320, 4, 3, 3], expected input[2, 8, 32, 32] to have 4 channels, but got 8 channels instead